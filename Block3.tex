\documentclass[usenames,dvipsnames]{beamer}
%
% Choose how your presentation looks.
%
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}  
\usepackage{tikz}%boxy  
\usetikzlibrary{arrows,positioning}
\usetikzlibrary{calc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{color}
\usepackage{hyperref}
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Darmstadt}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}  % or try default, serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{headline}{}
}
%
%
\newcommand{\mytikzmark}[2]{%
  \tikz[remember picture,inner sep=0pt,outer sep=0pt,baseline,anchor=base] 
    \node (#1) {\ensuremath{#2}};}
%
%
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw=Red,inner sep=2pt] (char) {#1};}}
%
\newcommand*\circledd[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw=ProcessBlue, dashed, inner sep=2pt] (char) {#1};}}
%
%
\newcommand*{\boxcolor}{Red}
\makeatletter
\renewcommand{\boxed}[1]{\textcolor{\boxcolor}{%
\tikz[baseline={([yshift=-1ex]current bounding box.center)}] \node [rectangle,semithick, minimum width=1ex,draw, dashed] {\normalcolor\m@th$\displaystyle#1$};}}
 \makeatother
%
%
\title[Block 1]{Block 3 \\  Panel data - models, estimation and testing }
\author{Advanced Econometrics 4EK608}
\institute{Vysoká škola ekonomická v Praze}
\date{}

\begin{document}
 
\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}
%
%---------------------------------
\section{Panel data -- basics (repetition from BSc courses)}
\begin{frame}{Panel data}
Panel data -- basics (repetition from BSc courses) \\ \bigskip
\begin{itemize}
\item Pooled cross sections
\bigskip
\item Longitudinal data
\bigskip
\item Panel data
\bigskip
\item Balanced \& unbalanced panel data sets
\bigskip
\item Dimensions of panel data sets \& analysis implications
\bigskip
\item Basic features and motivation for panel data use
\end{itemize}
\end{frame}
%---------------------------------
\subsection*{Pooled cross sections}
\begin{frame}{Pooled cross sections}
\begin{itemize}
\item \underline{\textbf{Pooled cross sections data:}}
Random sampling from a large population at different time periods (i.e. for each time period, we have a different - randomly chosen - set of CS units). 
\medskip
\item Should not be confused with ``actual'' panel data. 
\medskip
\item Pooled cross sections: sampling from a changing population at different points in time generates \textbf{independent, not identically distributed} (\textit{inid}) observations. 
\medskip
\item Pooled cross sections are easy to deal with, simply by allowing the intercept (and perhaps some selected slopes) in a LRM to vary across time. 
\medskip
\item Can be used for policy analysis \\(difference-in-differences estimator). 
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{Pooled cross sections}
\underline{\textbf{Pooled cross sections - model example}}
\begin{align*}
\log(\textit{wage}_{it}) & = \theta_0 + \theta_1 d91_t + \theta_2 d92_t + \delta_1 \textit{female}_{it} + \delta_2 \textit{educ}_{it} +\\
& + \gamma_1 \textit{exper}_{it} + \gamma_2 (\textit{female} \times d91)_{it} + \gamma_3 (\textit{female} \times d92)_{it} + u_{it} 
\end{align*}
\medskip
where $t = 1990, 1991, 1992$; \hspace{0.2cm} $i=1,2, \dots , \mytikzmark{500}{500}$ \\
$d91_{t}$ and $d92_t$ are time dummies, \\
\vspace{0.2cm}
$\textit{female}_{it}$, $\textit{educ}_{it}$ and $\textit{exper}_{it}$ describe the gender, education and work experience of the $i$-th individual at time $t$, \\
\vspace{0.2cm}
$(\textit{female} \times d91)_{it}$ is an interaction element, may be used to describe whether changes in wages over time are statistically different for man and woman.
\begin{tikzpicture}[<-,overlay,remember picture,inner sep=1.5pt,shorten <=0.2em,font=\tiny]
\tikzset{
    mynode/.style={rectangle,draw=ProcessBlue, fill=White, semithick, inner sep=.2em, minimum size=2em, text centered, text width=8em},
    myarrow/.style={->, >=stealth, thin, Red}
}
\node[mynode] at (4.7,3.3) (Box){Each year, we draw $500$ individuals at random. Individual respondents are not followed. Total observations: $N \times T = 1.500$};
  \draw[myarrow] (Box) -- ++   (500);
\end{tikzpicture}
\end{frame}
%---------------------------------
\begin{frame}{Pooled cross sections: Chow test}
\underline{\textbf{Pooled cross sections - model example contd.}}
\begin{align*}
 \log(\textit{wage}_{it}) =  \beta_0 & + \beta_1 d91_t + \beta_2 d92_t + \beta_3 \textit{female}_{it} + \\
 & + \beta_4 \textit{educ}_{it} + \beta_5 \textit{exper}_{it} + u_{it}
\end{align*}
\textbf{Chow test for structural changes across time} \\
\smallskip
Basically an \textit{F}-test for linear restrictions, can be used to determine whether the estimated slope coefficients change across time.\\
\medskip
In our $\log(wage)$ equation, we would test the $H_0$ of ``time-invariant'' $\beta_3, \beta_4$ and $\beta_5$ coefficients, while allowing for time dummies (time-specific intercepts).
\end{frame}
%---------------------------------
\begin{frame}{Pooled cross sections: Chow test}
\vspace{1cm}
\vfill
\bigskip
$F=\frac{ \mytikzmark{SSRr}{\textit{SSR}_r}- \mytikzmark{SSRur}{\textit{SSR}_{ur}}}{\textit{SSR}_{ur}} \cdot \frac{(n-T-Tk)}{(T-1)k};$ \\
\bigskip
{\small under $H_0$ of no structural break, $F \sim F((T-1)k, (n- \mytikzmark{TTk}{\circledd{$T-Tk$)}})$} \\
\bigskip
\begin{itemize}
\item [Note:] This test is not robust to heteroskedasticity (including changing variance across time). Robust variants of the test exist, based on interaction terms.
\end{itemize}
\begin{tikzpicture}[<-,overlay,remember picture,inner sep=1.5pt,shorten <=0.2em,font=\scriptsize]
\tikzset{
    mynode/.style={rectangle,draw=ProcessBlue, dashed, fill=White, semithick, inner sep=.2em, minimum size=2em, text centered, text width=9em},
    myarrow/.style={->, >=stealth, thin, ProcessBlue}
}
\node[mynode] at (1,6.2) (SSRr*){$\textit{SSR}_r$: restricted model – pooled regression, allowing for different time intercepts.};
\draw[myarrow] (SSRr*) -- ++   (SSRr);
	\node[mynode] at (5.5,6.2) (SSRur*){$\textit{SSR}_{ur}$:run a regression for each of the time periods. $\textit{SSR}_{ur} = 			\textit{SSR}_1 + \textit{SSR}_2 + \dots + \textit{SSR}_T$};
	\draw[myarrow] (SSRur*) -- ++   (SSRur);
		\node[mynode] at (9.3,6.2) (TTk*){$T + Tk$ parameters estimated in the unrestricted model};
		\draw[myarrow] (TTk*) -- ++   (TTk);
\end{tikzpicture}
\end{frame}
%---------------------------------
\subsection*{Longitudinal data}
\begin{frame}{Longitudinal data}
\footnotesize
\begin{itemize}
\item $N$ individual CS units are followed over time.
\medskip
\item Distinction between panel and longitudinal data may be subtle and different authors may use conflicting terminologies \dots
\medskip
\item For an observation $y_{it}$, $i$th individual is observed at a time period described by $t$. Number of observations in time may differ among CS units and observations may occur at different time periods. \\ \medskip 
\textbf{Example:} For a medical study, we measure child's weight (plus other data) at birth and repeatedly over a period of one year. For some $y_{it}$ observation, index $t$ denotes days from birth. Due to scheduling, individual children are weighted at different $t$ ``values''. Number of observations differ across children. Children in the study are born on different dates. \\ \medskip Example extends easily to micro-economic environment \\(we can follow newly founded companies, etc.).
\medskip
\item Longitudinal data are typically used in Linear mixed effects (LME) models (discussed separately).
\end{itemize}
\end{frame}
%---------------------------------
\subsection*{Panel data}
\begin{frame}{Panel data}
\begin{itemize}
\item In the dataset, $N$ individual CS units are followed over $T$ time periods. Typically, $t$ denotes a ``common'' time period (year, quarter, month) at which all CS units are observed if the panel is balanced.
\medskip
\item Regression model of the form $$y_i = \bm{x}_{it}^{\prime}\bm{\beta} + a_i + \varepsilon_{it},$$ where $i$ denotes CS units and $t$ identifies time periods, $a_i$ is the individual (company, group or other CS-unit) unobserved element. 
\medskip
\item In this course (Block 3), we focus on panel data.\\ \medskip Different data dimensions, model types, estimators and tests discussed next.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{Balanced \& unbalanced panel data sets}
\begin{itemize}
\bigskip
\item \textbf{Balanced panels:} observations available for all time periods on all CS units. Often assumed for simplicity of interpretation.
\bigskip
\item \textbf{Unbalanced panels:} mechanics of coefficient estimation do not differ. Model interpretation may require formal description of why the panel may be unbalanced.\\Does the random sampling assumption hold? 
\bigskip
\item Problems in unbalanced panels may be caused by: 
\medskip
\begin{itemize}
    \item \textbf{Sample selection bias:} with e.g. self-selection, coefficients can be be biased and inconsistent.
    \medskip
    \item \textbf{Attrition bias:} even if participants are randomly selected  at the beginning of observation, they often leave (medical study, school, etc.) on a non-random basis.   
\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{Dimensions of panel data sets}
\footnotesize
\begin{itemize}
\item Short panels: $N \gg T$ \\
Working with short panels is similar to CS data analysis. If CS units are randomly drawn from a population and $T$ is small and fixed, then asymptotic analysis (properties) hold for arbitrary time dependence and distributional heterogeneity across time.
\medskip
\item Long panels: $T \gg N$ \\
Working with long panels is similar to time-series analysis. In TS analysis, stationarity \& weak dependency conditions apply. SURE (Seemingly Unrelated Regression Equation) approach can be used: for the regression equations (with identical regressor structure), we estimate contemporaneous error covariances and use this information to improve efficiency of the estimate (see Greene, chapter 10.2)
\medskip
\item Large panel datasets: $T$ and $N$ large\\
Both CS and TS analysis assumptions apply, specialized estimators exist for large (heterogeneous) panels.\\Cointegrated series in panels: estimation and tests by Pesaran.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{Basic features and motivation for panel data use}
\textbf{Pooled regression with panel data:} \\ \medskip
\begin{itemize}
    \item Heterogeneity bias
    \medskip
    \item Similar principle to the Simpson's paradox
\end{itemize}
\medskip
\includegraphics[width=\textwidth]{./img/Obrazek4}
\end{frame}
%---------------------------------
\begin{frame}{Basic features and motivation for panel data use}
\textbf{Variation for the dependent variable and regressors}
\begin{itemize}
\item Overall variation: variation over time and individuals.
\item Between variation: variation between individuals.
\item Within variation: variation within individuals (over time).
\end{itemize}
\tiny
\begin{table}[]
\centering
\label{Tab3}
\noindent\makebox[\textwidth]{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Id  & Time & Variable & \begin{tabular}[c]{@{}c@{}}Individual\\ mean\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ mean\end{tabular} & \begin{tabular}[c]{@{}c@{}}Overall\\ deviation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Between\\ deviation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Within\\ deviation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Within\\ deviation\\ (modified)\end{tabular} \\ \hline
$i$ & $t$  & $x_{it}$ & $\overline{x}_i$ & $\overline{x}$ & $x_{it}-\overline{x}$ & $\overline{x}_{i}-\overline{x}$ & $x_{it}- \overline{x}_i$ & $x_{it}-\overline{x}_{i} + \overline{x}$ \\ \hline
1   & 1    & 9        & 10                                                        & 20                                                     & -11                                                         & -10                                                         & -1                                                         & 19                                                                      \\ \hline
1   & 2    & 10       & 10                                                        & 20                                                     & -10                                                         & -10                                                         & 0                                                          & 20                                                                      \\ \hline
1   & 3    & 11       & 10                                                        & 20                                                     & -9                                                          & -10                                                         & 1                                                          & 21                                                                      \\ \hline
2   & 1    & 20       & 20                                                        & 20                                                     & 0                                                           & 0                                                           & 0                                                          & 20                                                                      \\ \hline
2   & 2    & 20       & 20                                                        & 20                                                     & 0                                                           & 0                                                           & 0                                                          & 20                                                                      \\ \hline
2   & 3    & 20       & 20                                                        & 20                                                     & 0                                                           & 0                                                           & 0                                                          & 20                                                                      \\ \hline
3   & 1    & 25       & 30                                                        & 20                                                     & 5                                                           & 10                                                          & -5                                                         & 15                                                                      \\ \hline
3   & 2    & 30       & 30                                                        & 20                                                     & 10                                                          & 10                                                          & 0                                                          & 20                                                                      \\ \hline
3   & 3    & 35       & 30                                                        & 20                                                     & 15                                                          & 10                                                          & 5                                                          & 25                                                                      \\ \hline
\end{tabular}}
\end{table}
\end{frame}
%---------------------------------
\begin{frame}{Panel data models}
\underline{Panel data model -- a stylized and structured notation } \\
\medskip
$$y_{it} = \bm{g}_t \bm{\theta} + \bm{z}_i \bm{\delta} + \bm{w}_{it} \bm{\gamma} + a_i + u_{it}$$\\
\medskip
where \quad $i = 1,2, \dots, N$; $t = 1, 2, \dots, T$, \\
\medskip
$\bm{g}_t$ is a vector of aggregate time effects (often time dummies),\\
\medskip
$\bm{z}_i$ is a set of time-constant observed variables, \\
\medskip
$\bm{w}_{it}$ changes across $i$ and $t$ (for at least some units $i$ and time periods $t$), can include interactions among time-constant and time varying variables, \\
\medskip
$\bm{\theta, \delta}$ and $\bm{\gamma}$ -- regression coefficients 
\end{frame}
%---------------------------------
\begin{frame}{Panel data models}
\textbf{\underline{Panel data model - a structured notation example}} \\
\begin{align*}
\log(\textit{wage}_{it}) = \theta_0 & + \theta_1 d91_t + \theta_2 d92_t + \delta_1 \textit{female}_i + \delta_2 \textit{educ}_i +\\
& + \gamma_1 \textit{exper}_{it} + \gamma_2 (\textit{female} \times \textit{exper})_{it} + a_i + u_{it}
\end{align*}
Where $t = 1990, 1991, 1992$; \quad $i = 1, 2, \dots , \mytikzmark{100}{100}$. \\
For a balanced panel, $T \times N = 300$ \\
\vspace{0.2cm}
$d91_t$ and $d92_t$ are time dummies, \\
$\textit{female}_i$ and $\textit{educ}_i$ do not change over time \\(individuals in our dataset are not active students \dots ), \\
$\textit{exper}_{it}$ changes between individuals and across time periods,
$(\textit{female} \times \textit{exper})_{it}$ is an interaction element, changes between individuals and across time.
\begin{tikzpicture}[<-,overlay,remember picture,inner sep=1.5pt,shorten <=0.2em,font=\scriptsize]
\tikzset{
    mynode/.style={rectangle,draw=ProcessBlue,  fill=White, semithick, inner sep=.2em, minimum size=2em, text centered, text width=6em},
    myarrow/.style={->, >=stealth, thin, Red}
}
\node[mynode] at (4.9,3.44) (100*){We follow 100 individuals across three years.};
\draw[myarrow] (100*) -- ++   (100);
\end{tikzpicture}
\end{frame}
%---------------------------------
\section{Short panels -- estimation, inference \& testing}
\begin{frame}{Short panels -- estimation, inference \& testing}
    \begin{itemize}
        \item Estimation methods -- repetition from BSc courses
        \bigskip
        \item Choosing adequate estimators: assumptions and tests
        \bigskip
        \item Robust inference (autocorrelation and heteroscedasticity)
    \end{itemize}
\end{frame}
%---------------------------------
\subsection*{Estimation methods -- repetition from BSc courses}
\begin{frame}{LSDV regression}
In the model \quad  $y_{it} = \bm{x}_{it} \bm{\beta} + a_i + u_{it} \,$, \\
\bigskip
Elements $a_i$ are usually regarded as unobservable variables. \\
This approach gives appropriate interpretation of $\bm{\beta}$. \\
Traditional (old) approaches to fixed effects estimation view the $a_i$ as parameters to be estimated along with $\bm{\beta}$. \\
\bigskip
How to estimate $a_i$ values along with $\bm{\beta}$?\\
\medskip
\begin{itemize}
\item Define $N$ dummy variables - one for each cross-section. \\
(Amendment for dummy-variable trap is necessary.)
\smallskip
\item Convenient LSDV model expansion: use interactions to control for individual slopes for chosen regressors.
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{FD estimator}
We can eliminate unobserved individual heterogeneity from the regression: \quad $y_{it} = \bm{x}_{it} \bm{\beta} + a_i + u_{it}$ \\ \smallskip
by first differences (FD) transformation: \\
$\Delta y_{it} = y_{it} - y_{i,t-1} = \Delta \bm{x}_{it} \bm{\beta} + \Delta a_i + \Delta u_{it} = \Delta \bm{x}_{it} \bm{\beta} + \Delta u_{it}$ \\ \medskip
\begin{itemize}
\item[$\checkmark$] Removes any unobserved heterogeneity.
\item[$\times$] We remove all time-invariant factors in $\bm{x}$.\\
If the time-invariant regressors are of no interest, this is \\a robust estimator.
\end{itemize} \medskip
Estimation can be done with FGLS (autocorrelation of transformed residuals), or OLS with HAC robust errors. \\
\medskip
FD is most suitable when we have $t = 1; 2$ – two period panel (FD may be used with more time periods, we have $N(T-1)$ observations after differencing)
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{FD estimator – assumptions}
\begin{itemize}
\item[\textbf{FD.1}] Functional form: $y_{it} = \beta_1 x_{it1} + \dots + \beta_k x_{itk} + a_i + u_{it}$, $i = 1, \dots, N$, \ $t = 1, \dots, T$
\item[\textbf{FD.2}] We have random sample from cross-sectional units.
\item[\textbf{FD.3}] Each regressor changes in time at least for some $i$ and no perfect linear combination exists among regressors.
\item[\textbf{FD.4}] For each $i$ and $t$, \ $E (u_{it} \mid \bm{X}_i, a_i) = 0$. [Alt.: regressors are strictly exogenous conditional on unobserved effects: $\textnormal{corr}(x_{itj}, u_{is} \mid a_i)=0$, \quad $\forall \ t, s$]
\item[\textbf{FD.5}] Variance of differenced errors conditional on all regressors is constant: $\textnormal{var}(\Delta u_{it} \mid \bm{X}_i) = \sigma^2$, \quad $t= 2,3, \dots, T$. [homoscedasticity]
\item[\textbf{FD.6}] No serial correlation exists among differenced errors. $\textit{cov}(\Delta u_{it}, \Delta u_{is} \mid \bm{X}_i) = 0$, \quad $t \neq s$
\item[\textbf{FD.7}] Differenced errors are normally distributed conditional on all regressors $\bm{X}_i$.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{FD estimator – assumptions}
Under  \textcolor{blue}{\textbf{FD.1 - FD.4}}\\
FD estimator is unbiased. \\
FD estimator is consistent for fixed $T$ as $N \rightarrow \infty$.\\
For unbiasedness, $E (\Delta u_{it} \mid \bm{X}_i) = 0$ (for $t = 2,3, \dots$) is sufficient (instead of FD.4)\\
\medskip
Under \textcolor{blue}{\textbf{FD.1 - FD.6}}\\
FD estimator is BLUE (conditional on explanatory variables).\\
Asymptotic inference for FD estimator holds ($t$ and $F$ statistics asymptotically follow corresponding distributions).\\
\medskip
Under  \textcolor{blue}{\textbf{FD.1 - FD.7}}\\
FD estimator is BLUE (conditional on explanatory variables).\\
FD estimators - i.e. pooled OLS on first differences - are normally distributed ($t$ and $F$ statistics have exact $t$ and $F$ distributions).
\end{frame}
%---------------------------------
\begin{frame}{FD estimator}
\underline{\textbf{Problems related to the FD estimator:}}\\ \medskip
\begin{itemize}
\item First-differenced estimates will be imprecise if explanatory variables vary only to a small extent over time (no estimate possible if regressors are time-invariant).
\item Potentially, there is insufficient (lower) variability in differenced variables.
\item Without strict exogeneity of regressors (e.g. in the case of a lagged dependent variable /say, $y_{i,t-1}$/ among regressors or with measurement errors), adding further periods does not reduce inconsistency.
\item FD estimator may be worse than pooled OLS if explanatory variables are subject to measurement errors (errors in variables - EIV).
\end{itemize}
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{FE estimator}
``Fixed'' means correlation of $a_i$ and $\bm{x}_{it}$, not that $a_i$ is non-stochastic.\\ \medskip
We can rewrite $y_{it} = \bm{x}_{it} \bm{\beta} + a_i + u_{it}$ as follows:\\
$y_{it} = \beta_1 x_{it1} + \dots + \beta_k x_{itk} + a_i + u_{it},$$
\hfill $$i = 1, \dots, N$, \ $t = 1, \dots,T$ \\ 
Now, \underline{for each $i$}, we \underline{average} the above equation \underline{over time}:\\
\medskip
${\overline{y}_i = \beta_1 \overline{x}_{i1} + \dots + \beta_k \overline{x}_{ik} + \overline{a}_i + \overline{u}_i}$\\ ($N$ equations with individual averages)\\
\medskip
By subtracting individual averages from the original observations (time-demeaning), we get:\\ \medskip
$\Rightarrow \boxed{[y_{it} - \overline{y}_{i}]} = \beta_1 \boxed{[x_{it1}-\overline{x}_{i1}]}+\dots+\beta_k \boxed{[x_{itk}-\overline{x}_{ik}]}+\boxed{[u_{it}- \overline{u}_i]}$\\
\medskip
Alternative notation: $\ddot{y}_{it} = \bm{\ddot{x}}_{it} \bm{\beta} + \ddot{u}_{it}$; where $\ddot{y}_{it} = y_{it} - \overline{y}_{i}$, etc.\\
\medskip
FE estimator, denoted $\bm{\hat{\beta}}_{FE}$, is the pooled OLS estimator applied to time-demeaned data.
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{FE estimator}
\textbf{FE estimator:} by time demeaning, we get rid of the $a_i$ element - as it does not vary over time 
\medskip
\begin{itemize}
\item $a_i = \overline{a}_i \ \rightarrow \ a_i - \overline{a}_i = 0$
\item Intercept and all time-invariant regressors are also eliminated using the FE (within) transformation.
\end{itemize}
\medskip
After FE estimation, $a_i$ elements may be estimated as follows:\\ \medskip
$\hat{a}_i =\overline{y}_i - \hat{\beta}_1 \overline{x}_{i1} - \dots - \hat{\beta}_k \overline{x}_{ik},$ \ $i = 1, \dots, N$ \\
\medskip
However, in most practical applications, $a_i$ values bear limited useful information.\\ \medskip
For each C-S observation $i$, we loose one d.f. in estimation ~ \dots for each $i$, the demeaned errors $\ddot{u}_{it}$ add up to zero when summed over time. Hence \ $df = N(T-1)-k$
\end{frame}
%---------------------------------------------------------------------
\begin{frame}{FE estimator – assumptions}
\begin{itemize}
\item[\textbf{FE.1}] Functional form: $y_{it} = \beta_1 x_{it1} + \dots + \beta_k x_{itk} + a_i + u_{it}$, $i = 1, \dots, N$, \ $t = 1, \dots, T$
\item[\textbf{FE.2}] We have random sample from cross-sectional units.
\item[\textbf{FE.3}] Each regressor changes in time at least for some $i$ and no perfect linear combination exists among regressors.
\item[\textbf{FE.4}] For each $i$ and $t$, \ $E (u_{it} \mid \bm{X}_i, a_i) = 0$. [Alt.: regressors are strictly exogenous conditional on unobserved effects: $\textnormal{corr}(x_{itj}, u_{is} \mid a_i)=0$, \quad $\forall \ t, s$]
\item[\textbf{FE.5}] Variance of errors conditional on all regressors is constant: $\textnormal{var}(u_{it} \mid \bm{X}_i, a_i) = \textnormal{var}(u_{it}) = \sigma^2_u$, \quad $t= 1,2, \dots, T$. [homoscedasticity]
\item[\textbf{FE.6}] No serial correlation exists among idiosyncratic errors. $\textnormal{cov}(u_{it}, u_{is} \mid \bm{X}_i, a_i) = 0$, \quad $t \neq s$
\item[\textbf{FE.7}] Errors are normally distributed conditional on all regressors $(\bm{X}_i, a_i)$.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{FE estimator – assumptions}
Under \textcolor{blue}{\textbf{FE.1 - FE.4}} (identical to  \textcolor{blue}{\textbf{FD.1 - FD.4}})\\
FE estimator is unbiased. \\
FE estimator is consistent for fixed $T$ as $N \rightarrow \infty$.\\
\vspace{0.5cm}
Under \textcolor{blue}{\textbf{FE.1 - FE.6}}\\
FE estimator is BLUE.\\
FD is unbiased\\ \dots  \textcolor{blue}{\textbf{FE.6}} makes FE better (less variance) than FD.\\
Asymptotically valid inference for FE estimator holds ($t$ and $F$).\\
\vspace{0.5cm}
Under  \textcolor{blue}{\textbf{FE.1 - FE.7}}\\
FE estimator is BLUE and $t$ and $F$ statistics have exact $t$ and $F$ distributions.\\
FE estimators - i.e. pooled OLS on time demeaned data - are normally distributed.
\end{frame}
%---------------------------------
\begin{frame}{RE estimator}
\small 
If $a_i$ are uncorrelated with $\bm{x}_{it}$, then it may be appropriate to model the individual constant terms as randomly distributed across cross-sectional units. RE models are appropriate if C-S units are from \textbf{a large sample} (good asymptotic properties).
\bigskip

\begin{itemize}
\item RE models reduce the number of parameters estimated.
\item RE estimator potentially inconsistent if assumptions not met.
\item $y_{it} = \bm{x}_{it} \bm{\beta} + a_i + u_{it}$\\ \medskip
If we can assume that $a_i$ is uncorrelated with each explanatory variable: $\textnormal{corr}(\bm{x}_{it}, a_i) = 0$; \ $t = 1,2, \dots, T$ \\then we may drop $a_i$ from the equation and $\beta_j$ estimates will remain unbiased.\\
\item By dropping $a_i$ from the regression, we effectively create a new error term: $v_{it} = a_i + u_{it}$\\
\medskip
\item As $a_i$ is time-invariant, the random element $v_{it}$ contains a lot of ``inertia'', i.e. autocorrelation (unless $a_i = 0$).
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{RE estimator - FGLS}
\small 
$y_{it} = \beta_0 + \beta_1 x_{it1} + \dots + \beta_k x_{itk} + v_{it};$\\
\bigskip
The quasi-demeaning (quasi-differencing) parameter $\theta$ is used for the FGLS estimation:\\ \medskip
$\theta = 1 - \big[ \sigma^2_u / (\sigma^2_u + T \sigma^2_{\mu}) \big]^{1/2}$, \quad  $0 \le \theta \le 1$\\ \medskip
where $\textit{var}(a_i) = \sigma^2_{\mu}$; \quad $\textit{var}(u_i) = \sigma^2_u$\\
\begin{itemize}
\item \small For each dataset, consistent estimators of $\sigma^2_{\mu}$ and $\sigma^2_u$ are available.\\
\item \small Their estimation is based on pooled OLS or FE. \\ Also, we use the fact that $\sigma^2_v = \sigma^2_{\mu} + \sigma^2_u$
\end{itemize}
RE estimator is a pooled OLS used on the quasi-demeaned data:\\ \medskip
{\footnotesize $$\boxed{[y_{it} - \theta \overline{y}_i]} = \beta_1 \boxed{[x_{it1}- \theta \overline{x}_{i1}]}+\dots+\beta_k \boxed{[x_{itk}- \theta \overline{x}_{ik}]}+\boxed{[a_i - \theta \overline{a}_i + u_{it}-\theta \overline{u}_i]}$$} \\ \medskip
(transformed errors follow G-M assumptions -- not autocorrelated)
\end{frame}
%---------------------------------
\begin{frame}{RE estimator - FGLS}
\
{\footnotesize $$\boxed{[y_{it} - \theta \overline{y}_i]} = \beta_1 \boxed{[x_{it1}- \theta \overline{x}_{i1}]}+\dots+\beta_k \boxed{[x_{itk}- \theta \overline{x}_{ik}]}+\boxed{[a_i - \theta \overline{a}_i + u_{it}-\theta \overline{u}_i]}$$}\\
\bigskip
Interestingly, the FGLS equation is a general form that encompasses both FE and pooled OLS:
\bigskip
\begin{align*}
\hat{\theta} \rightarrow 1 \quad & \rightarrow \quad \textnormal{RE}  \rightarrow \ \textnormal{FE}\\
\hat{\theta} \rightarrow 0 \quad & \rightarrow \quad \textnormal{RE}  \rightarrow \ \textnormal{Pooled}
\end{align*}
\end{frame}
%---------------------------------
\begin{frame}{RE estimator – Assumptions}
\footnotesize
\begin{itemize}
\item[\textbf{FE.1}] Functional form: $y_{it} = \beta_1 x_{it1} + \dots + \beta_k x_{itk} + a_i + u_{it}$, $i = 1, \dots, N$, \ $t = 1, \dots, T$
\item[\textbf{FE.2}] We have random sample from cross-sectional units.
\item[\textbf{FE.4}] $\forall \ i, t$: \ $E (u_{it} \mid \bm{X}_i, a_i) = 0$. [Alt.: $\textnormal{corr}(x_{itj}, u_{is} \mid a_i)=0$, \ $\forall \ t, s$]
\item[\textbf{FE.5}] Variance of idiosyncratic errors conditional on all regressors is constant: $\textnormal{var}(u_{it} \mid \bm{X}_i, a_i) = \textnormal{var}(u_{it}) = \sigma^2_u$, \quad $t= 1,2, \dots, T$. [homoscedasticity]
\item[\textbf{FE.6}] No serial correlation exists among idiosyncratic errors. $\textnormal{cov}(u_{it}, u_{is} \mid \bm{X}_i, a_i) = 0$, \quad $t \neq s$
\item[\textcolor{black}{\textbf{FE.7}}] [small sample normality of $u_{it}$ has little importance for RE estimator]
\end{itemize}
	\begin{tikzpicture}
	\draw[thick, color=blue] (0,0) -- (10.5,0);
	\end{tikzpicture}
\begin{itemize}
\item[\textbf{RE.1}] There are no perfect linear relationships among explanatory variables. [replaces \textcolor{blue}{\textbf{FE.3}}]
\item[\textbf{RE.2}] In addition to \textcolor{blue}{\textbf{FE.4}}, the expected value of $a_i$ given all regressors is constant: $E(a_i \mid \bm{X}_i)=\beta_0$. [Rules out correlation between $a_i$ and $\bm{X}_i$]
\item[\textbf{RE.3}] In addition to \textcolor{blue}{\textbf{FE.5}}, variance of $a_i$ given all regressors is constant: $\textnormal{var}(a_i \mid \bm{X}_i)=\sigma^2_a$ [homoscedasticity imposed on $a_i$]
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{RE estimator – Assumptions}
Under  \textcolor{blue}{\textbf{FE.1+FE.2+RE.1+(FE.4+RE.2)}}\\
RE estimator is consistent and asymptotically normal \\(for fixed $T$ as $N \rightarrow \infty$).\\
RE standard errors and statistics are not valid unless \textcolor{blue}{\textbf{(FE.5+RE.3)}} and  \textcolor{blue}{\textbf{FE.6}} conditions are met.\\
\bigskip
Under  \textcolor{blue}{\textbf{FE.1-FE.2+RE.1+(FE.4+RE.2)+(FE.5+RE.3)+FE.6}}\\
RE estimator is consistent and asymptotically normal \\(for fixed $T$ as $N \rightarrow \infty$).\\
RE standard errors and statistics are valid.\\
RE is asymptotically efficient 
\begin{itemize}
\item[-] lower st.errs. than pooled OLS
\item[-] for time-varying variables, RE estimator is more efficient than FE (FE cannot be used on time-invariant variables).
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{CRE estimator}
Correlated Random Effects (CRE) estimator - a synthesis of the RE and FE approaches: 
\vspace{0.5cm}
\begin{itemize}
\item $a_i$ viewed as random, yet they can be correlated with $\bm{x}_{it}$.\\
\vspace{0.2cm}
Specifically, as $a_i$ do not vary over time, it makes sense to allow for their correlation with the time average of $x_{it}:\overline{x}_i = T^{-1} \sum^T_{t=1}x_{it}$
\vspace{0.2cm}
\item CRE allows for incorporation of time-invariant regressors (compare to FE).
\vspace{0.2cm}
\item CRE allows for convenient testing of FE vs. RE.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{CRE estimator}
CRE: The individual-specific effect $a_i$ is split up into a part that is related to the time-averages of the explanatory variables and a part $r_i$ (a time-constant unobservable) that is unrelated to the explanatory variables: 
\begin{align*}
\textnormal{For } y_{it} & =  \beta_1 x_{it} + a_i + u_{it}, \\
&\textnormal{~~we assume (a single-regressor illustration):}\\ 
a_i & = \alpha + \gamma \overline{x}_i + r_i \textnormal{,~~now: } \textnormal{corr}(r_i, \overline{x}_i) = 0 \Rightarrow \textnormal{corr}(r_i,x_{it}) = 0\\ 
&\textnormal{~~(because $\overline{x}_i$  is a linear function of  $x_{it}$)}
\end{align*}

By substituting for $a_i$ into the first equation, we obtain: \\
$y_{it} = \alpha + \beta_1 x_{it} + \gamma \overline{x}_i + r_i + u_{it}$ \\
\bigskip
\underline{This equation can be estimated using RE}\\
As $\gamma \overline{x}_i$ controls for the correlation between $a_i$ and $x_{it}$, \\$r_i$ is uncorrelated with regressors.
\end{frame}
%---------------------------------
\begin{frame}{CRE estimator}
CRE: \ $y_{it} = \alpha + \beta_1 x_{it} + \gamma \overline{x}_i + r_i + u_{it}$ \\
\medskip
\small CRE is a modified RE of the original equation $y_{it} =  \beta_1 x_{it} + a_i + u_{it}$: \\
\vspace{0.2cm}
with uncorrelated random effect $r_i$ but with the time averages as additional regressors. \\
\vspace{0.3cm}
\underline{The resulting CRE estimate for $\beta$ is identical to the FE estimator}. \\ \medskip
CRE allows for convenient testing of FE vs. RE:
	\begin{itemize}
	\item[$H_0$:] $\gamma = 0$ can be evaluated using $\hat{\gamma}_{\textit{CRE}}$ and appropriate (HCE) standard errors against
	\item[$H_1$:] $\gamma \neq 0$ [RE assumes $\gamma = 0$: reject $H_0\rightarrow$ reject RE in favor of FE]
	\end{itemize}

\begin{itemize}
    \item CRE is a versatile estimator. In terms of model specification, it allows for incorporation of time-invariant regressors into panel data models where $a_i$ is correlated with regressors.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{Arellano-Bond estimator (dynamic panels)}
Dynamic panel model example:\\
\medskip
$y_{it} = \delta_1 y_{i,t-1} + \bm{x}^{\prime}_{it} \bm{\beta} + a_i + u_{it}$\\
\medskip
\dots may be expanded using additional lags of the dependent variable or using lagged exogenous regressors.\\
\medskip
\small
\textbf{Nickel Bias}
\begin{itemize}
\item Related (mostly) to the lagged exogenous regressors $\bm{x}$
\item FEs take up some part of the dynamic effect and therefore dynamic panel data models lead to overestimated FEs and underestimated dynamic interactions. 
\item Whether the Nickel bias is significant in a particular model/dataset situation is an empirical question. Nevertheless, in theory this bias persists unless the number of time observations goes to infinity.
\item The inclusion of additional cross-sections to the dataset would worsen the bias in most cases.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{Arellano-Bond estimator (dynamic panels)}
\textbf{Arellano-Bond (AB) estimator} \\ \medskip
\begin{itemize}
\item The model is transformed into first differences to eliminate the individual effects:\\
$\Delta y_{it} = \delta_1 \Delta y_{i,t-1} + \Delta \bm{x}^{\prime}_{it} \bm{\beta} + \Delta u_{it}$, 
\medskip
\item then a generalized method of moments (GMM) approach is used to produce asymptotically efficient estimates of the coefficients.
\medskip
\item AB is based on IVR (we need instruments for lagged dependent variable as this is an endogenous regressor in the FD-transformed model ($\Delta y_{i,t-1}$ correlated to $\Delta u_{it}$).
\medskip
\item \textcolor{red}{Warning:} AR(2) / not AR(1) / autocorrelation in residuals of the AB-estimated model renders the AB estimator inconsistent. After using the AB estimator, always test for AR(2) autocorrelation in the residuals!
\end{itemize}
\end{frame}
%---------------------------------
\subsection*{Choosing adequate estimators -- assumptions and tests}
\begin{frame}{Choosing adequate estimators -- assumptions and tests}
    
\end{frame}

































%---------------------------------
\begin{frame}{FE vs FD estimator}
\begin{itemize}
\item For $T=2$, FE and FD estimators produce identical estimates and inference. (FE must include a time dummy for the second period to be actually identical to the FD estimation output)
\item For $T>2$, FE and FD are both unbiased under FE.1 - FE.4. Both FE and FD are consistent for fixed $T$ as $N \rightarrow \infty$
\item If $u_{it}$ is not serially correlated, FE is more efficient than FD
\item If $u_{it}$ follows a random walk (hence $\Delta u_{it}$ is serially uncorrelated) FD is better than FE.
\item If $u_{it}$ shows some level of positive serial correlation (not a random walk), FD and FE may not be easily compared. For negative correlation of $u_{it}$, we prefer FE.
\end{itemize}
\end{frame}
%---------------------------------
\begin{frame}{FE vs FD estimator}
\begin{itemize}
\item For $T \gg N$, especially if non-stationary series are involved, FE may lead to spurious regression problems, while the FD help us transforming integrated series into weakly dependent series.
\item If strict exogeneity is violated, both FE and FD are biased. However, FE is likely to have less bias than FD (unless $T=2$). The bias of FD does not depend on $T$, while the bias in FE tends to zero at rate $1/T$.
\item \dots it may be a good idea to use both FD and FE. If the results are not method-sensitive, so much the better. If the results from FE and FD differ significantly, we sometimes report both.
\end{itemize}
\end{frame}
%---------------------------------




















%---------------------------------
\section{Long panels -- estimation \& inference}
\begin{frame}{Frame Title}
    \begin{itemize}
        \item Estimation methods - repetition from BSc courses
        \bigskip
        \item Choosing estimators: assumptions and tests
        \bigskip
        \item Robust inference (autocorrelation and heteroscedasticity)
    \end{itemize}
\end{frame}
%---------------------------------











%---------------------------------
\section{Large panel data sets -- introduction to estimators and tests}
\begin{frame}{Frame Title}
    \begin{itemize}
        \item Estimation methods - repetition from BSc courses
        \bigskip
        \item Choosing estimators: assumptions and tests
        \bigskip
        \item Robust inference (autocorrelation and heteroscedasticity)
    \end{itemize}
\end{frame}
%---------------------------------










%---------------------------------
\section{Panel data -- advanced topics and extensions}
\begin{frame}{Panel data - Extensions}
\begin{itemize}
\item Advanced (PhD level) course on panel data \\
\textsubscript{\textcolor{Blue}{\url{http://people.stern.nyu.edu/wgreene/Econometrics/PanelDataNotes.htm}} }
\bigskip
\item Mixed effects model \\
Extension to the RE model \\(intercept and -some- coefficients have a random term):
$$
y_{it}=\bm{x}_{it}^{\prime} \bm{\beta} + \bm{z}_{it}^{\prime}( \bm{\gamma} + \bm{h}_i) +
(\alpha + u_i) + \varepsilon_{it}
$$
where $\bm{h}_i$ describes random variation of the paremeter(s) across individuals.\\
\medskip

\textsubscript{ \textcolor{Blue}{\url{http://www.bodowinter.com/tutorial/bw_LME_tutorial1.pdf}} } \\
\end{itemize}
\end{frame}
%---------------------------------
\end{document}