\documentclass{beamer}
%
% Choose how your presentation looks.
%
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}  
\usepackage{amsmath}
\usepackage{bm}
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Darmstadt}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}  % or try default, serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{headline}{}
}
%
%
\title[Week1]{Block 1 \\  Repetition from BSc courses \\ Time series regression models \\ Non-linear extensions to LRMs}
\author{Advanced Econometrics 4EK608}
\institute{Vysoká škola ekonomická v Praze}
\date{}

\begin{document}
 
\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------
\section{Estimation methods, predictions from a model}
\subsection{Ordinary least squares}
\begin{frame}{Linear regression model (LRM) and OLS estimation}
$$
\bm{y} = \bm{X\beta} + \bm{\varepsilon}
$$
\textbf{LRM assumptions (for OLS estimation):}\\
(Notation follows Greene, Econometric analysis, $7^{\textnormal{th}}$ ed.)
\medskip
\begin{enumerate}
    \item[A1] \textbf{Linearity:} $y_i = \beta_1 + \beta_2 x_{i2} + \dots + \beta_K x_{iK} + \varepsilon_i$ \\LRM describes linear relationship between $y_i$ and $\bm{x}_i$.
    \item[A2] \textbf{Full rank:} Matrix $\bm{X}$ is an $n \! \times \! K$ matrix with rank $K$.\\ Columns of $\bm{X}$ are linearly independent and $n \geq K$.
    \item[A3] \textbf{Exogeneity of regressors:} $E[\varepsilon_i | \bm{X}]=0$ (strict form). \\If relaxed to contemporaneous form in TS: $E[\varepsilon_t | \bm{x}_t]=0$.\\Law of iterated expectations: $E[\varepsilon_i | \bm{X}]=0 ~\Rightarrow~ E[\varepsilon]=0$.\\Also, we assume disturbances convey no information on each other: $E[\varepsilon_i|\varepsilon_1,\dots,\varepsilon_{i-1},\varepsilon_{i+1},\dots,\varepsilon_n]=0$.
\end{enumerate}
\end{frame}
%------------------------------------------------------    
\begin{frame}{Linear regression model (LRM) and OLS estimation}
$$
\bm{y} = \bm{X\beta} + \bm{\varepsilon}
$$
\textbf{LRM assumptions (continued):}
\medskip
\begin{enumerate}
    \item[A4] \textbf{Homoskedastic \& nonautocorrelated disturbances:}$$E[\bm{\varepsilon\varepsilon}^{\prime}]=\sigma^2\bm{I}$$ Homoscedasticity: $\textnormal{var}[\varepsilon_i|\bm{X}]=\sigma^2, \qquad \forall~i=1,\dots,n$.\\Independent disturbances: $\textnormal{cov}[\varepsilon_t,\varepsilon_s|\bm{X}]=0, \qquad \forall~t \neq s$.\\ \smallskip GARCH models (i.e. $\textnormal{var}[\varepsilon_t|\varepsilon_{t-1}]=\sigma^2+\alpha \varepsilon_{t-1}$) do not violate the conditional variance assumption, but $\textnormal{var}[\varepsilon_t|\varepsilon_{t-1}] \neq \textnormal{var}[\varepsilon_t]$.
    \item[A5] \textbf{DGP of \textit{X}:} Variables in $\bm{X}$ may be fixed or random.
    \item[A6] \textbf{Normal distribution of disturbances:} $$\varepsilon | \bm{X} \sim N[\bm{0}, \sigma^2\bm{I}].$$
\end{enumerate}
\end{frame}
%------------------------------------------------------
\begin{frame}{Ordinary least squares (OLS)}
\vspace{-0.3cm}
$$
\bm{y} = \bm{X\beta} + \bm{\varepsilon}
$$
\medskip
The least squares estimator is unbiased (given A1 -- A3):
\begin{align*}
    \hat{\bm{\beta}}=green\bm{b}&=(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{y}~=~\bm{\beta}+(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{\varepsilon}, \\
    &\textnormal{take expectations, iterating over}~\bm{X}:\\
    E[\bm{b}|\bm{X}]&= \bm{\beta}+E[(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{\varepsilon}|\bm{X}]~=~\bm{\beta}.\\
\end{align*}

Variance of the least squares estimator (A1 -- A4):
\begin{align*}
    \textnormal{var}[\bm{b}|\bm{X}]&=E[(\bm{b}-\bm{\beta})(\bm{b}-\bm{\beta})^{\prime}|\bm{X}]\\
    &=E[\bm{A \varepsilon \varepsilon}^{\prime}\bm{A}^{\prime}|\bm{X}] \textnormal{~~where~~}\bm{A}=(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\\
    &= \sigma^2 (\bm{X}^{\prime}\bm{X})^{-1}.\\
\end{align*}

Normal distribution of the least squares estimator (A1 -- A6):
$$\bm{b} | \bm{X} \sim N[\bm{\beta}, \sigma^2(\bm{X}^{\prime}\bm{X})^{-1}].$$
    
\end{frame}
%------------------------------------------------------
\subsection{General properties of estimators}
\begin{frame}{Estimators and estimation methods}
\begin{itemize}
    \item LRM is not the only type of regression model.
    \bigskip
    \item OLS is not the only useful estimator.
    \bigskip
    \item Let's approach estimators and their properties more generally.\\~\\
    (again, notation follows Greene, Econometric analysis.)
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Notation/definitions:
\begin{itemize}
\item $\bm{x}_j = (x_{1j},\dots,x_{nj})^{\prime}$ - random sample of $n$ observations.
\item $\bm{\theta}$ - population parameter [unknown parameter(s)]
\item $f(\bm{x}_j,\bm{\theta})$: probability distribution function
\item $\hat{\bm{\theta}}$ is some estimator of $\bm{\theta}$
\end{itemize}
\medskip
Basic notions: 
\begin{itemize}
\item All estimators have sampling distributions\\
~~mean: $E(\hat{\theta})$\\
~~variance: $E[(\hat{\theta}-E(\hat{\theta}))^2]$, etc.\\
\item Estimators $\times$ estimate 
\item Generally, many estimators exist for a given parameter. \\Population mean example:
\end{itemize}
\begin{align*}
\hat{\theta}_1 & = \overline{x} = \frac{\sum_{i=1}^nx_i}{n}\\
\hat{\theta}_2 & = \tilde{x} = \frac{1}{2}(x_{max} +x_{min})
\end{align*}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Properties of estimators - classification:
\medskip
\begin{itemize}
\item \textbf{Unbiasedness}: can be described as $E(\hat{\bm{\theta}})= \bm{\theta}$. \\Rarely useful in finite (small) sample context. Even asymptotically (large sample), discussion would be directed towards consistency (far more desirable feature).
\medskip
\item \textbf{Consistency:} $\textnormal{plim}(\hat{\bm{\theta}}) = \bm{\theta}$. \\Holds if $\hat{\bm{\theta}}$ is an unbiased estimator of $\bm{\theta}$ and $\textnormal{plim}(\textnormal{var}(\hat{\bm{\theta}}))=\bm{0}$ ~i.e. [$\textnormal{var}(\hat{\bm{\theta}}) \rightarrow \bm{0} \textnormal{ as } n \rightarrow \infty$].
\begin{itemize}
\item Consistent estimators: unbiased \& their variance shrinks to zero as sample size grows (entire population is used).
\item Minimal requirement for estimator used in statistics or econometrics.
\item If some estimator is not consistent, then it does not provide estimates of population $\theta$ values, even with unlimited data.
\item Unbiased estimators are not necessarily consistent. 
\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Properties of estimators - classification:
\medskip
\begin{itemize}
\item \textbf{Efficiency, asymptotic efficiency:} an estimator is efficient if it is unbiased and no other unbiased estimator has a smaller variance. Often difficult to prove, we usually simplify the concept to \textit{relative efficiency} (e.g.: efficiency with respect to linear unbiased estimators, etc.). Asymptotic efficiency: Holds for an estimator that is asymptotically unbiased and no other asymptotically unbiased estimator has smaller asymptotic variance.
\medskip
\item \textbf{Normality, asymptotic normality:} basis for most statistical inference performed with common estimators. 
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\textbf{Extremum estimator:} obtained as the optimizer of some criterion function $q(\bm{\theta}|\textbf{data})$. Most common estimators:
\medskip
\begin{itemize}
    \item[LS] $\hat{\bm{\theta}}_{\textit{LS}}~~~=\textnormal{argmax}\left[ -\frac{1}{n} \displaystyle\sum_{i=1}^n (y_i - h(\bm{x}_i,\bm{\theta}_{\textit{LS}}))^2\right]$,
    \item[ML] $\hat{\bm{\theta}}_{\textit{ML}}~~\,=\textnormal{argmax}\left[ \frac{1}{n} \displaystyle\sum_{i=1}^n \log f(y_i|\bm{x}_i,\bm{\theta}_{\textit{ML}})\right]$,
    \item[GMM] $\hat{\bm{\theta}}_{\textit{GMM}}=\textnormal{argmax}\,\left[ -\overline{\bm{m}}(\textbf{data},\bm{\theta}_{\textit{GMM}})^{\prime}\bm{W}\overline{\bm{m}} (\textbf{data},\bm{\theta}_{\textit{GMM}})\right]$,
\end{itemize}
\medskip
where $h(\cdot)$ is a function (linear/non-linear $\rightarrow$ OLS/NLS), \\$\overline{\bm{m}}$ denotes sample moments and $\bm{W}$ is a convenient positive definite matrix (discussed next).\\
\medskip
LS and ML estimators belong to a class of \textbf{M estimators} \\(``M'' for maximum likelihood-type).
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Assumptions for asymptotic properties of extremum estimators:
\medskip
\begin{itemize}
    \item[1] \textbf{Parameter space:} must be convex and the parameter vector that is the object of estimation must be point in its interior. Gaps and nonconvexities in parameter spaces would generally collide with estimation algorithms \\(settings such as $\sigma^2 > 0$ are OK).
    \medskip
    \item[2] \textbf{Criterion function:} must be concave in the parameters (concave in the neighborhood of the true parameter vector). Criterion functions need not be globally concave. In such situation, there may be multiple optima (often associated with poor model specification).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Assumptions for asymptotic properties of extremum estimators:
\medskip
\begin{itemize}
    \item[3] \textbf{Identifiability of parameters:} has a relatively complex technical definition (anything like ``true parameters $\bm{\theta}_0$ are identified if...'' is problematic - leads to a paradox if condition is not met). Simple way to secure identification:
    \medskip
    \begin{itemize}
        \item \textbf{LS:} for a given set of $\bm{x}_i$ observations, any two different parameter vectors $\bm{\theta}$ and $\bm{\theta}_0$ must lead to different conditional mean function ($\hat{y}_i$).
        \smallskip
        \item \textbf{ML:} For any data vector $(y_i, \bm{x}_i)$ and two parameter vectors $\bm{\theta} \neq \bm{\theta}_0$, it must be possible to produce different values of the density function $f(y_i|\bm{x}_i,\bm{\theta})$. Note: identifiability does not rule out the  $f(y_i|\bm{x}_i,\bm{\theta}) = f(y_i|\bm{x}_{\ell},\bm{\theta}), ~~i \neq \ell$ \\situation (A2 full rank condition must be observed).
        \smallskip
        \item \textbf{GMM:} sufficient condition for identification:\\ $E[\overline{\bm{m}}(\textbf{data},\bm{\theta})] \neq \bm{0}$ if $\bm{\theta} \neq \bm{\theta}_0$.
    \end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Assumptions for asymptotic properties of extremum estimators:
\medskip
\begin{itemize}
    \item[4] \textbf{Behavior of the data:} Grenander conditions for well-behaved data:
    \medskip
    \begin{itemize}
        \item[G1] For each $\bm{x}_k$ column of $\bm{X}$ and $d_{nk}^2 = \bm{x}_k^{\prime}\bm{x}_k$, it must hold that: $\lim_{n \rightarrow \infty} d_{nk}^2 = + \infty$. Sum of squares continue to grow with sample size ($\bm{x}_k$ does not degenerate into a series of 0).
        \smallskip
        \item[G2] The $\lim_{n \rightarrow \infty} x_{ik}^2 / d_{nk}^2 = 0$ for all $i=1,2,\dots,n$. Single observations become less important as sample size grows. No single observation will dominate $\bm{x}_k^{\prime}\bm{x}_k$.
        \smallskip
        \item[G3] Let $\bm{C}_n$ be sample correlation matrix of the columns in $\bm{X}$ (excluding the intercept, if present). Then $\lim_{n \rightarrow \infty} \bm{C}_n = \bm{C}$ where $\bm{C}$ is positive definite. This implies that the full rank condition for $\bm{X}$ (A2) is not asymptotically violated.
    \end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\begin{block}{Theorem: Consistency of M estimators}
If: \\ \smallskip 
\begin{itemize}
    \item[(a)] the parameter space is convex and the true parameter vector is a point in its interior,
    \smallskip
    \item[(b)] the criterion function is concave,
    \smallskip
    \item[(c)] the parameters are identified by the criterion function,
    \smallskip
    \item[(d)] the data are well behaved,\\ \smallskip
\end{itemize}
then the M estimator converges in probability to the true parameter vector. 
\end{block}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\small 
\begin{block}{Theorem: Asymptotic normality of M estimators}
If:
\begin{itemize}
    \item[(a)] $\hat{\bm{\theta}}$ is a consistent estimator of $\bm{\theta}_0$ where $\bm{\theta}_0$ is a point in the interior of the parameter space $\bm{\Theta}$,
    \smallskip 
    \item[(b)] $q(\bm{\theta}|\textbf{data})$ is concave and twice continuously differentiable in $\bm{\theta}$ in a neighborhood of $\bm{\theta}_0$,  
    \smallskip 
    \item[(c)] $\sqrt{n} \left[ \partial q(\bm{\theta}_0|\textbf{data})/ \partial \bm{\theta}_0 \right] \xrightarrow{~~d~~} N(\bm{0},\bm{\Phi})$,  
    \smallskip 
    \item[(d)] $\underset{n \rightarrow \infty}{\lim} \textnormal{Pr} \left[ |(\partial^2 q(\bm{\theta}|\textbf{data})/ \partial \theta_k \partial \theta_m)- h_{km}(\bm{\theta}) | >\varepsilon \right] = 0~ \forall \varepsilon > 0$ for any $\bm{\theta}$ in $\bm{\Theta}$; $h_{km}(\bm{\theta})$ is a continuous finite valued function of $\bm{\theta}$,
    \smallskip 
    \item[(e)] the matrix of elements $\bm{H}(\bm{\theta})$ is nonsingular at $\bm{\theta}_0$,
\end{itemize}
\smallskip then $\sqrt{n} (\hat{\bm{\theta}} - \bm{\theta}_0) \xrightarrow{~~d~~} N \lbrace \bm{0},[ \bm{H}^{-1}(\bm{\theta}_0)\bm{\Phi}\bm{H}^{-1}(\bm{\theta}_0)] \rbrace$. 
\end{block}
where $\bm{\Phi}$ is a variance-covariance matrix,   \\and $\bm{H}(\bm{\theta}_0)=\partial^2 q(\bm{\theta}|\textbf{data})/ \partial \bm{\theta} \partial \bm{\theta}^{\prime}$ is a Hessian (evaluated at $\bm{\theta}_0$).
\end{frame}
%---------------------------------------------
\subsection{Method of moments}
\begin{frame}{Method of moments}
\begin{itemize}
    \item Method of moments (MM)
    \bigskip
    \item Generalized method of moments (GMM)
\end{itemize} 
\end{frame}
%---------------------------------------------
\begin{frame}{Method of moments}

\begin{itemize}
\item With the method of moments, we simply estimate population moments by corresponding sample moments. 
\medskip
\item Under very general conditions, sample moments are consistent estimators of the corresponding population moments, but NOT necessarily unbiased estimators.
\end{itemize}
\begin{block}{Application example 1}
Sample covariance is a consistent estimator of population covariance.
\end{block}
\begin{block}{Application example 2}
OLS estimators we have used for parameters in the CLRM can be derived by the method of moments. 
\end{block}
\end{frame}
%---------------------------------------------
\begin{frame}{Method of moments}
\textbf{Method of moments (MM)}\\
\smallskip
\underline{Population moments} for a stochastic variable $X$\\
\begin{itemize}
\item $E(X^r)$: $r^{th}$ population moment about zero
\item $E(X)$: the population mean: $1^{\textnormal{st}}$ sample moment about zero
\item $E[(X-E(X))^2]$: the population variance is the second moment about the mean
\end{itemize}
\medskip
\underline{Sample moments} for sample observations $(x_1, x_2, \dots,x_n)$
\begin{itemize}
\item $\frac{\sum_{i=1}^n x^r_i}{n}$: $r^{th}$ sample moment about zero
\item $\frac{\sum_{i=1}^n x_i}{n}=\overline{x} $ : sample mean is the first moment about zero
\item $\frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n-1}$: sample variance is the second sample moment about the mean
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Method of moments}
%\footnotesize
\begin{itemize}
%\item In a LRM: $\, \hat{y} = \hat{\beta}_1 + \hat{\beta}_2 x_{2} + \dots + \hat{\beta}_K x_{K} \,$, the $K$ parameters can be OLS-estimated by minimizing:

%\begin{equation*}
%\sum_{i=1}^n \left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right)^2
%\end{equation*}
\item For MM, the usual linear model assumption (concerning $1^{\textnormal{st}}$ population moment) $E[\bm{x}_i \varepsilon_i]=\bm{0}$ implies orthogonality condition: $$\textnormal{cov}[\bm{x}_i, \varepsilon_i] = \bm{0} \textnormal{~~i.e.~~}E[\bm{x}_i(y_i-\bm{x}_i^{\prime}\bm{\beta}]=\bm{0},$$ 
may be transformed in \textbf{population moment equation:}
$$
E \left[ \frac{1}{n} \, \sum_{i=1}^n \bm{x}_i (y_i - \bm{x}_i^{\prime}\bm{\beta}) \right] 
= E \left[ \overline{\bm{m}}(\bm{\beta}) \right] = \bm{0}\,,
$$
and corresponding sample (empirical) moment equation:
$$
\left[ \frac{1}{n} \sum_{i=1}^n \bm{x}_i (y_i - \bm{x}_i^{\prime}\hat{\bm{\beta}}) \right]
= \overline{\bm{m}}(\hat{\bm{\beta}}) = \bm{0}.
$$
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Method of moments}
The equation form of MM empirical equations can be produced as:
\medskip
\footnotesize
\begin{equation*}
\begin{aligned}
\frac{1}{n}\, \sum_{i=1}^n ~~~~\left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right) &= 0\\
\frac{1}{n}\, \sum_{i=1}^n  x_{i1} \left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right) &= 0\\
\dots &\\
\frac{1}{n}\, \sum_{i=1}^n x_{iK} \left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right) &= 0\\
\end{aligned}
\end{equation*}
\medskip
\begin{itemize}
    \item The $\frac{1}{n}$ element can be removed here.
    \item This is a system of $K$ equations with $K$ unknown parameters.
    \item Equivalent to $1^{st}$ order conditions for the OLS estimator:
    $$ \underset{\hat{\bm{\beta}}}{\min}\sum_{i=1}^n \left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right)^2$$
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}
\begin{itemize}
    \item GMM is a very general class of estimators, includes most of other estimators as a special case (IVR, simultaneous equations, Arellano-Bond estimator for dynamic panels). 
    \medskip
    \item For single equation linear models, GMM may be conveniently described using the instrumental variable case:\\ \bigskip For the LRM $y_i = \bm{x}_i^{\prime}\bm{\beta} + \varepsilon_i$, 
    \begin{itemize}
    \medskip
        \item  we abandon the assumption $E[\bm{x}_i^{\prime} \varepsilon_i]=0$ and
        \medskip
        \item we replace it by $E[\bm{z}_i^{\prime} \varepsilon_i]=0$.
        \medskip
        \item Hence, columns of $\bm{X}~(n\!\times\!K)$ are potentially endogenous \\ \medskip and $\bm{Z}~(n\!\times\!L)$ is a matrix of exogenous instruments.
    \end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}

\begin{itemize}
\item GMM equation can be cast by analogy to the MM case: we start by $E[\bm{z}_i \varepsilon_i]=\bm{0}$, which implies orthogonality condition: $$\textnormal{cov}[\bm{z}_i, \varepsilon_i] = \bm{0} \textnormal{~~i.e.~~}E[\bm{z}_i(y_i-\bm{x}_i^{\prime}\bm{\beta}]=\bm{0}.$$ 
and \textbf{population moment equation:}
$$
E \left[ \frac{1}{n} \, \sum_{i=1}^n \bm{z}_i (y_i - \bm{x}_i^{\prime}\bm{\beta}) \right] 
= E \left[ \overline{\mathbf{m}}(\bm{\beta}) \right] = \bm{0}\,,
$$
and corresponding sample (empirical) moment equation:
$$
\left[ \frac{1}{n} \sum_{i=1}^n \bm{z}_i (y_i - \bm{x}_i^{\prime}\hat{\bm{\beta}}) \right]
= \overline{\mathbf{m}}(\hat{\bm{\beta}}) = \bm{0}.
$$
\end{itemize}

\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}
The equation form of GMM empirical equations can be produced as:
\medskip
\footnotesize
\begin{equation*}
\begin{aligned}
\frac{1}{n}\, \sum_{i=1}^n ~~~~\left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right) &= 0\\
\frac{1}{n}\, \sum_{i=1}^n  z_{i1} \left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right) &= 0\\
\dots &\\
\frac{1}{n}\, \sum_{i=1}^n z_{iL} \left( y_i - \hat{\beta}_1 - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_K x_{iK} \right) &= 0\\
\end{aligned}
\end{equation*}
\medskip
\begin{itemize}
    \item First column of $\bm{Z}$ is assumed to be a vector of ones (same as for $\bm{X}$).
    \smallskip
    \item For $\bm{Z}=\bm{X}$ as a special case, the above equations are identical to MM (shown previously) and the solution is identical to the OLS estimator: $\hat{\bm{\beta}}=(\bm{X}^{\prime}\bm{X})^{-1} \bm{X}^{\prime}\bm{y}$.
    \smallskip
    \item For $\bm{Z} \neq \bm{X}$, where $\bm{Z}$ is $(n\! \times \!L)$ and $\bm{X}$ is $(n \! \times \!K)$, three identification possibilities have to be considered. 
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}
\textbf{Identification of GMM equations}
\bigskip
\begin{itemize}
    \item[1] \textbf{Underidentified:} with $L < K$, there are fewer moment equations than unknown parameters ($\beta_j$). Without additional information (parameter restrictions), there is no solution to the system of GMM equations.
    \bigskip
    \item[2] \textbf{Exactly identified:} for $L = K$, single solution exists:  
    \begin{equation*}
        \begin{aligned}
        \left[ \frac{1}{n} \sum_{i=1}^n \bm{z}_i (y_i - \bm{x}_i^{\prime}\hat{\bm{\beta}}) \right]
&= \overline{\mathbf{m}}(\hat{\bm{\beta}}) = \bm{0},\\
\textnormal{can be convenie} & \textnormal{ntly~re-written as:}\\
\overline{\mathbf{m}}(\hat{\bm{\beta}}) &= \left( \frac{1}{n} \bm{Z}^{\prime} \bm{y} \right) - \left( \frac{1}{n} \bm{Z}^{\prime} \bm{X} \right) \hat{\bm{\beta}} = \bm{0}\\
\textnormal{and the solution} & \textnormal{~yields the familiar IV estimator:}\\
\hat{\bm{\beta}} &=     (\bm{Z}^{\prime}\bm{X})^{-1} \bm{Z}^{\prime}\bm{y}.
        \end{aligned}
    \end{equation*}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}
\textbf{Identification of GMM equations} (continued)
\bigskip
\begin{itemize}
    \item[3] With $L > K$, there is no unique solution to the equation system $\overline{\mathbf{m}}(\hat{\bm{\beta}}) = \bm{0}$. \\
    \medskip
    One intuitite solution is the ``least squares approach'':
    $$\underset{\bm{\beta}}{\min}\left(  \overline{\mathbf{m}}(\hat{\bm{\beta}})^{\prime} \overline{\mathbf{m}}(\hat{\bm{\beta}})  \right)$$
    Through the first order conditions, we obtain a GMM estimator as
    $$
    \hat{\bm{\beta}} =\left[ (\bm{X}^{\prime}\bm{Z}) (\bm{Z}^{\prime}\bm{X}) \right]^{-1} (\bm{X}^{\prime}\bm{Z}) \bm{Z}^{\prime}\bm{y}.
    $$
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}
\textbf{GMM - consistency conditions}\\

\medskip
\begin{itemize}
  \item \textbf{Convergence of the moments:} Empirical (sample) moments converge in probability to their population counterparts. DGP meets the conditions for LLN. \\ \smallskip
  $\overline{\mathbf{m}}(\bm{\beta}) = \frac{1}{n} \left(  \bm{Z}^{\prime} \bm{y} -  \bm{Z}^{\prime} \bm{X}  \bm{\beta} \right) \xrightarrow{p} \bm{0}$.
  \medskip
  \item \textbf{Identification:} For any $n \geq K$ and $\bm{\beta}_1 \neq \bm{\beta}_2$ it holds that $\overline{\mathbf{m}}(\bm{\beta}_1) \neq \overline{\mathbf{m}}(\bm{\beta}_2)$. Three implications:\\
  \medskip
  \begin{itemize}
      \item \textbf{Order condition:} $L \geq K$. Number of moment equations \\at least as large as number of parameters.
      \smallskip
      \item \textbf{Rank condition:} matrix $\bm{G}(\bm{\beta})=\partial \, \overline{\mathbf{m}}(\bm{\beta}) / \partial \bm{\beta}^{\prime}$ (i.e. $\frac{1}{n}\bm{Z}^{\prime} \bm{X}$) is a $L\! \times \! K$ matrix with row rank equal to $K$. Moment conditions are not redundant (implies order condition).
      \smallskip
      \item \textbf{Uniqueness:} unique solution/optimizer exists.
  \end{itemize}
\medskip
\item \textbf{Limiting Normal distribution for the sample moments:} Population moments obey central limit theorem (CLT) or some similar variant.
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Generalized method of moments}
\textbf{GMM - final remarks \& summary}\\
\begin{itemize}
\item GMM-based asymptotic covariance matrix of $\hat{\bm{\beta}}$ is discussed in Greene (Econometric analysis, ch. 13.6) for the classical, heteroscedastic and generalized case (includes TS-based estimation).
\medskip
\item GMM is robust to differences in ``specification'' of the data generating process (DGP). $\rightarrow$ i.e. sample mean or sample variance estimate their population counterparts (assuming they exist) regardless of DGP.
\medskip
\item GMM is free from distributional assumptions. ``Cost'' of this approach: if we know the specific distribution of a DGP, GMM does not make use of such information $\rightarrow$ inefficient estimates.
\medskip 
\item Alternative approach: method of maximum likelihood utilizes distributional information and is more efficient (provided this information is valid).
\end{itemize}
\end{frame}










%---------------------------------------------
\subsection{Maximum likelihood estimation}
\begin{frame}{Estimators and Estimation Methods}
\textbf{Maximum likelihood estimator}\\
\medskip
Single $\theta$ parameter case:\\
\begin{itemize}
\item $1^{st}$ step: deriving a likelihood function $L=L(\theta ,y_1, y_2, ... , y_n)$, where $y_i$ is observation of $Y$ (stochastic), $\theta$ is parameter of the distribution.\\
\item $2^{nd}$ step: finding maximum of $L$ with respect to $\theta$, \\that maximum is $\tilde{\theta} = \theta_{MLE}$
\end{itemize}
With more parameters: $\bm{\theta} = (\theta_1, \dots , \theta_m)^{\prime}$
$$L=L(\theta_1, \theta_2, ... \theta_m, y_1, y_2, ... , y_n)$$
We find MLEs of the $m$ parameters by partially differentiating the likelihood function $L$ with respect to each $\theta$ and then setting all the partial derivatives obtained to zero.\\
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Likelihood function:\\ 
$f(y_1,y_2,\dots,y_n|\bm{\theta}) = \prod_{i=1}^n f(y_i|\bm{\theta}) = L(\bm{\theta}|\bm{y})$ \\
where $f(y|\bm{\theta})$ is the pdf of $y$, conditioned on set of parameters $\bm{\theta}$.\\
\medskip
\centerline{\underline{Maximum likelihood estimation of CLRM parameters:}}
\begin{align*}
\textnormal{CLRM: } y_i= \alpha + \beta x_i + \varepsilon_i \qquad \mathbf{E}(y_i) & =\alpha +\beta x_i = \bm{x}_i^{\prime}\bm{\beta}\\
\textit{var}(y_i) & =\textit{var}(\varepsilon_i)=\sigma^2
\end{align*}
Probability density function for \textbf{Normal distribution}:\\
\medskip
$f(y|\bm{\theta})=(2\pi\sigma^2)^{-0.5} \hspace{0.1cm} exp[-(y-\mu)^2/2\sigma^2]$\\
\bigskip
In the case of CLRM, for each $y_i=\alpha + \beta x_i + \varepsilon_i$: \\
\medskip
$f(y_i| \bm{x}_i,\bm{\theta})=(2\pi\sigma^2)^{-0,5} \hspace{0.1cm} exp[-(y_i-\mathbf{E}(y_i))^2/2\sigma^2]$, that is:\\
$f(y_i| \bm{x}_i,\bm{\theta})=(2\pi\sigma^2)^{-0,5} \hspace{0.1cm} exp[-(y_i-\bm{x}_i^{\prime}\bm{\beta})^2/2\sigma^2]$

\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Log-likehood ($LL$) function, \textbf{Normal distribution} assumed,\\
estimation of CLRM parameters:\\
\begin{align*}
log L(\bm{\theta}|\bm{y}, \bm{X} ) & =\sum_{i=1}^n log[f(y_i|\bm{x}_i, \bm{\theta})] = \\
& = -\frac{1}{2} \sum_{i=1}^n \left\lbrace { log(2\pi) + log(\sigma^2)+\frac{1}{\sigma^2}[y_i-\bm{x}_i^{\prime}\bm{\beta}]^2} \right\rbrace = \\ 
& = -\frac{n}{2}log(2\pi)-\frac{n}{2}log(\sigma^2)-\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i-\bm{x}_i^{\prime}\bm{\beta}]^2
\end{align*}
\\ \bigskip
numerical iterative method is used for $\bm{\theta}=(\alpha, \beta, \sigma^2)$ estimation\\
(by maximizing the log-likelihood function).\\
if CLRM assumptions hold $\Rightarrow$ MLE estimators $\tilde{\alpha}$, $\tilde{\beta}$ and $\tilde{\sigma}^2$\\ are identical to OLS-generated estimators $\hat{\alpha}$, $\hat{\beta}$ (and $\hat{\sigma}^2$).
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and Estimation Methods}
\textbf{Basic MLE assumptions}\\
\begin{itemize}
    \item \textbf{Parameter space:} Gaps and nonconvexities in parameter spaces would generally collide with estimation algorithms (settings such as $\sigma^2 > 0$ are OK).
    \item \textbf{Identifiability:} The parameter vector $\bm{\theta}$ is identified (estimable), if for two vectors, $\bm{\theta}^{*} \neq \bm{\theta}$ and for some data observations $\bm{x}$, $L(\bm{\theta}^{*}|\bm{x}) \neq L(\bm{\theta}|\bm{x})$.
    \item \textbf{Well-behaved data:} Laws of large numbers (LLN) apply. Some form of CLT can be applied to the gradient (i.e. for the estimation method).
    \item \textbf{Regularity conditions:} ``well behaved'' derivatives of $f(y_i|\bm{\theta})$ with respect to $\bm{\theta}$ (see Greene, chapter 14.4.1).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and Estimation Methods}
\textbf{MLE properties}\\
\begin{itemize}
    \item \textbf{Consistency:} $\textnormal{plim}(\hat{\bm{\theta}}) = \bm{\theta}_0$ ~~($\bm{\theta}_0$ is the true parameter)
    \medskip
    \item \textbf{Asymptotic normality} of $\bm{\hat{\theta}}$
    \medskip
    \item \textbf{Asymptotic efficiency:}  $\bm{\hat{\theta}}$ is asymptotically efficient and achieves the Cramér-Rao lower bound for consistent estimators (see Greene, chapter 14.4.5)
    \medskip
    \item \textbf{Invariance:} MLE of $\bm{\gamma}_0=\bm{c}(\bm{\theta}_0)$ is $\bm{c}(\bm{\hat{\theta}})$ if $\bm{c}(\bm{\theta}_0)$ \\is a continuous and countinuously differentiable function.\\ \smallskip \footnotesize{ 
    (empirical advantages: we can use reparameterization in MLE, \\~e.g. $\gamma_j = 1/\theta_j$ ~~or~~ $\theta^2 = 1/\sigma^2$).}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\textbf{MLE - summary}\\
\begin{itemize}
\item MLE is only possible if we know the form of the probability distribution function for the population (Normal, Poisson, Negative Binomial, etc.).
\medskip
\item MLEs possess the large sample properties of consistency and asymptotic efficiency. There is no guarantee that they possess any desirable small-sample properties. 
\medskip
\item Under CLRM assumptions, MLE estimator are identical to OLS estimators.
\medskip
\item MLE-related tests (Likelihood ratio, Wald, LM) will be discussed separately, with reference to a specific model type (e.g. LDVs in Weeks 11 to 13).
\end{itemize}
\end{frame}
%---------------------------------------------
\subsection{Predictions from a model}
\begin{frame}{Predictions from a model}
    
\end{frame}
%---------------------------------------------
\section{Non-linear extensions to LRM, quantile regression}

\begin{frame}{Introduction}

\begin{itemize}
  \item 
\end{itemize}

\end{frame}
%------------------------------------------------------
\section{TS-based regression models, non/stationarity, cointegration}

\begin{frame}{Introduction}

\begin{itemize}
  \item 
\end{itemize}

\end{frame}
%------------------------------------------------------

\end{document}
